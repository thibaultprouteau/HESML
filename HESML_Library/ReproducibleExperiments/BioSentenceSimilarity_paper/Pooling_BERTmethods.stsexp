<?xml version="1.0" encoding="UTF-8"?>

<!-- edited with XMLSPY v5 U (http://www.xmlspy.com) by Ramon Yepes (Investronica) -->
<!-- This experiment evaluates the impact of stop words in string-based STS measures -->
<SentenceSimilarityExperiments xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../SentenceSimilarityExperiments.xsd">
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>Pooling_BERTMethods/raw_similarity_BIOSSES.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>BIOSSESNormalized.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>

			<!--<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed NONE</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>NONE</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5555</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure> -->
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MEAN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5557</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5559</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MEAN_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5561</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed FIRST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>FIRST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5563</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed LAST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>LAST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5565</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<!--<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 NONE</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>NONE</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5567</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>-->
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MEAN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5569</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5571</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MEAN_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5573</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 FIRST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>FIRST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5575</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 LAST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>LAST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5577</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>Pooling_BERTMethods/raw_similarity_MedSTSFull.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>MedStsFullNormalized.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>

			<!--<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed NONE</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>NONE</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5555</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>-->
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MEAN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5557</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5559</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MEAN_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5561</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed FIRST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>FIRST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5563</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed LAST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>LAST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5565</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<!--<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 NONE</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>NONE</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5567</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>-->
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MEAN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5569</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5571</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MEAN_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5573</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 FIRST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>FIRST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5575</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 LAST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>LAST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5577</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>Pooling_BERTMethods/raw_similarity_CTR.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>CTRNormalized_averagedScore.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>

			<!--<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed NONE</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>NONE</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5555</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>-->
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MEAN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5557</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5559</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed REDUCE_MEAN_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5561</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed FIRST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>FIRST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5563</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed LAST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>LAST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5565</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<!--<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 NONE</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>NONE</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5567</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>-->
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MEAN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5569</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5571</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 REDUCE_MEAN_MAX</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN_MAX</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5573</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 FIRST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>FIRST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5575</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12 LAST_TOKEN</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>LAST_TOKEN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5577</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/PretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
</SentenceSimilarityExperiments>