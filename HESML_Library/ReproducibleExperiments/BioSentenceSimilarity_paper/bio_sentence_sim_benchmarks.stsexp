<?xml version="1.0" encoding="UTF-8"?>
<!-- edited with XMLSPY v5 U (http://www.xmlspy.com) by Ramon Yepes (Investronica) -->
<SentenceSimilarityExperiments xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../SentenceSimilarityExperiments.xsd">
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>BioSentenceSimFinalRawOutputFiles/raw_similarity_BIOSSES.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>BIOSSESNormalized.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Jaccard</Method>
				<Label>Jaccard</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Qgram</Method>
				<Label>Qgram</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>BlockDistance</Method>
				<Label>BlockDistance</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>OverlapCoefficient</Method>
				<Label>OverlapCoefficient</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Levenshtein</Method>
				<Label>Levenshtein</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bio_embedding_extrinsic</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioWordVec_extrinsic</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bio_embedding_intrinsic</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioWordVec_intrinsic</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_defaultchar.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_defaultchar</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_Defaultchar200dim_8ene.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_Defaultchar200dim_8ene</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_Nonechar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_Nonechar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>stanford_skipgram_nonechar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>stanford_skipgram_nonechar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>stanf_skipgram_defaultchar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>stanf_skipgram_defaultchar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Average</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>BioNLP2016_PubMed-shuffle-win-2.bin</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioNLP2016_PubMed-shuffle-win-2</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Average</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>BioNLP2016_PubMed-shuffle-win-30.bin</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioNLP2016_PubMed-shuffle-win-30</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<USEModelMeasure>
				<Label>universal-sentence-encoder-4</Label>
				<Method>USEModel</Method>
				<PretrainedModelURL>https://tfhub.dev/google/universal-sentence-encoder/4</PretrainedModelURL>
				<PythonScriptsDirectory>../UniversalSentenceEncoderExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../UniversalSentenceEncoderExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractUniversalSentenceEncoderVectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</USEModelMeasure>
			<USEModelMeasure>
				<Label>universal-sentence-encoder-4</Label>
				<Method>USEModel</Method>
				<PretrainedModelURL>https://tfhub.dev/google/universal-sentence-encoder/4</PretrainedModelURL>
				<PythonScriptsDirectory>../UniversalSentenceEncoderExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../UniversalSentenceEncoderExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractUniversalSentenceEncoderVectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</USEModelMeasure>
			<Sent2vecModelMeasure>
				<Label>sent2vec BioSentVec</Label>
				<Method>Sent2vecModel</Method>
				<PretrainedModelDir>../SentenceEmbeddings/</PretrainedModelDir>
				<PretrainedModelFile>BioSentVec_PubMed_MIMICIII-bigram_d700.bin</PretrainedModelFile>
				<PythonScriptsDirectory>../Sent2vecExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../Sent2vecExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractSent2vecvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</Sent2vecModelMeasure>
			<Sent2vecModelMeasure>
				<Label>sent2vec BioSentVec2</Label>
				<Method>Sent2vecModel</Method>
				<PretrainedModelDir>../SentenceEmbeddings/</PretrainedModelDir>
				<PretrainedModelFile>BioSentVec_PubMed_MIMICIII-bigram_d700.bin</PretrainedModelFile>
				<PythonScriptsDirectory>../Sent2vecExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../Sent2vecExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractSent2vecvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</Sent2vecModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5555</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pmc</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pmc</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5557</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pmc</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed_pmc</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed_pmc</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5559</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed_pmc</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5561</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5563</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5565</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5567</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<WBSMMeasure>
				<Label>WBSM Lin</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Lin</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM Cai</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Cai</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<!--<WBSMMeasure>
				<Label>WBSM Rada</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Rada</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure> -->
			<WBSMMeasure>
				<Label>WBSM Resnik</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Resnik</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM WuPalmerFast</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>WuPalmerFast</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM JiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>JiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM CosNormJConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>CosineNormJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<!--<WBSMMeasure>
				<Label>WBSM WeightedJiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>WeightedJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM CosineNormWeightedJiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>CosineNormWeightedJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>-->
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>BioSentenceSimFinalRawOutputFiles/raw_similarity_MedSTSFull.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>MedStsFullNormalized.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Jaccard</Method>
				<Label>Jaccard</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Qgram</Method>
				<Label>Qgram</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>BlockDistance</Method>
				<Label>BlockDistance</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>OverlapCoefficient</Method>
				<Label>OverlapCoefficient</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Levenshtein</Method>
				<Label>Levenshtein</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bio_embedding_extrinsic</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioWordVec_extrinsic</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bio_embedding_intrinsic</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioWordVec_intrinsic</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_defaultchar.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_defaultchar</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_Defaultchar200dim_8ene.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_Defaultchar200dim_8ene</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_Nonechar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_Nonechar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>stanford_skipgram_nonechar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>stanford_skipgram_nonechar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>stanf_skipgram_defaultchar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>stanf_skipgram_defaultchar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Average</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>BioNLP2016_PubMed-shuffle-win-2.bin</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioNLP2016_PubMed-shuffle-win-2</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Average</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>BioNLP2016_PubMed-shuffle-win-30.bin</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioNLP2016_PubMed-shuffle-win-30</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<USEModelMeasure>
				<Label>universal-sentence-encoder-4</Label>
				<Method>USEModel</Method>
				<PretrainedModelURL>https://tfhub.dev/google/universal-sentence-encoder/4</PretrainedModelURL>
				<PythonScriptsDirectory>../UniversalSentenceEncoderExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../UniversalSentenceEncoderExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractUniversalSentenceEncoderVectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</USEModelMeasure>
			<USEModelMeasure>
				<Label>universal-sentence-encoder-4</Label>
				<Method>USEModel</Method>
				<PretrainedModelURL>https://tfhub.dev/google/universal-sentence-encoder/4</PretrainedModelURL>
				<PythonScriptsDirectory>../UniversalSentenceEncoderExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../UniversalSentenceEncoderExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractUniversalSentenceEncoderVectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</USEModelMeasure>
			<Sent2vecModelMeasure>
				<Label>sent2vec BioSentVec</Label>
				<Method>Sent2vecModel</Method>
				<PretrainedModelDir>../SentenceEmbeddings/</PretrainedModelDir>
				<PretrainedModelFile>BioSentVec_PubMed_MIMICIII-bigram_d700.bin</PretrainedModelFile>
				<PythonScriptsDirectory>../Sent2vecExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../Sent2vecExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractSent2vecvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</Sent2vecModelMeasure>
			<Sent2vecModelMeasure>
				<Label>sent2vec BioSentVec2</Label>
				<Method>Sent2vecModel</Method>
				<PretrainedModelDir>../SentenceEmbeddings/</PretrainedModelDir>
				<PretrainedModelFile>BioSentVec_PubMed_MIMICIII-bigram_d700.bin</PretrainedModelFile>
				<PythonScriptsDirectory>../Sent2vecExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../Sent2vecExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractSent2vecvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</Sent2vecModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5571</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pmc</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pmc</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5573</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pmc</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed_pmc</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed_pmc</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5575</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed_pmc</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5577</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5579</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5581</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5583</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<WBSMMeasure>
				<Label>WBSM Lin</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Lin</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM Cai</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Cai</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<!--<WBSMMeasure>
				<Label>WBSM Rada</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Rada</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure> -->
			<WBSMMeasure>
				<Label>WBSM Resnik</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Resnik</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM WuPalmerFast</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>WuPalmerFast</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM JiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>JiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM CosNormJConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>CosineNormJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<!--<WBSMMeasure>
				<Label>WBSM WeightedJiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>WeightedJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM CosineNormWeightedJiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>CosineNormWeightedJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>-->
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>BioSentenceSimFinalRawOutputFiles/raw_similarity_CTR.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>CTRNormalized_averagedScore.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Jaccard</Method>
				<Label>Jaccard</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Qgram</Method>
				<Label>Qgram</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>BlockDistance</Method>
				<Label>BlockDistance</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>OverlapCoefficient</Method>
				<Label>OverlapCoefficient</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<StringBasedSentenceSimilarityMeasure>
				<Method>Levenshtein</Method>
				<Label>Levenshtein</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</StringBasedSentenceSimilarityMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bio_embedding_extrinsic</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioWordVec_extrinsic</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bio_embedding_intrinsic</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioWordVec_intrinsic</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_defaultchar.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_defaultchar</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_Defaultchar200dim_8ene.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_Defaultchar200dim_8ene</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>bioc_skipgram_Nonechar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>bioc_skipgram_Nonechar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>stanford_skipgram_nonechar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>stanford_skipgram_nonechar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Max</Pooling>
				<WordEmbeddingFileFormat>FastTextVecWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>stanf_skipgram_defaultchar200dim.vec</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>stanf_skipgram_defaultchar200dim</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Average</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>BioNLP2016_PubMed-shuffle-win-2.bin</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioNLP2016_PubMed-shuffle-win-2</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<SWEMMeasure>
				<Pooling>Average</Pooling>
				<WordEmbeddingFileFormat>BioWordVecBinaryWordEmbedding</WordEmbeddingFileFormat>
				<PretrainedModelFilename>BioNLP2016_PubMed-shuffle-win-30.bin</PretrainedModelFilename>
				<PretrainedModelDirectory>../WordEmbeddings</PretrainedModelDirectory>
				<Label>BioNLP2016_PubMed-shuffle-win-30</Label>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>BioCNLPTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</SWEMMeasure>
			<USEModelMeasure>
				<Label>universal-sentence-encoder-4</Label>
				<Method>USEModel</Method>
				<PretrainedModelURL>https://tfhub.dev/google/universal-sentence-encoder/4</PretrainedModelURL>
				<PythonScriptsDirectory>../UniversalSentenceEncoderExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../UniversalSentenceEncoderExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractUniversalSentenceEncoderVectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</USEModelMeasure>
			<USEModelMeasure>
				<Label>universal-sentence-encoder-4</Label>
				<Method>USEModel</Method>
				<PretrainedModelURL>https://tfhub.dev/google/universal-sentence-encoder/4</PretrainedModelURL>
				<PythonScriptsDirectory>../UniversalSentenceEncoderExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../UniversalSentenceEncoderExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractUniversalSentenceEncoderVectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</USEModelMeasure>
			<Sent2vecModelMeasure>
				<Label>sent2vec BioSentVec</Label>
				<Method>Sent2vecModel</Method>
				<PretrainedModelDir>../SentenceEmbeddings/</PretrainedModelDir>
				<PretrainedModelFile>BioSentVec_PubMed_MIMICIII-bigram_d700.bin</PretrainedModelFile>
				<PythonScriptsDirectory>../Sent2vecExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../Sent2vecExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractSent2vecvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>Default</CharFilteringType>
				</WordProcessing>
			</Sent2vecModelMeasure>
			<Sent2vecModelMeasure>
				<Label>sent2vec BioSentVec2</Label>
				<Method>Sent2vecModel</Method>
				<PretrainedModelDir>../SentenceEmbeddings/</PretrainedModelDir>
				<PretrainedModelFile>BioSentVec_PubMed_MIMICIII-bigram_d700.bin</PretrainedModelFile>
				<PythonScriptsDirectory>../Sent2vecExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../Sent2vecExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractSent2vecvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>NoneStopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>None</CharFilteringType>
				</WordProcessing>
			</Sent2vecModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5585</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pmc</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pmc</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5587</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pmc</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.0_pubmed_pmc</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.0_pubmed_pmc</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5589</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.0_pubmed_pmc</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5591</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5593</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5595</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_uncased_L-12_H-768_A-12</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<BertEmbeddingModelMeasure>
				<Label>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</Label>
				<Method>BERTEmbeddingModel</Method>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<PythonServerPort>5597</PythonServerPort>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python3</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>NCBI_BERT_pubmed_uncased_L-24_H-1024_A-16</PretrainedModelFilename>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			<WBSMMeasure>
				<Label>WBSM Lin</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Lin</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM Cai</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Cai</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<!--<WBSMMeasure>
				<Label>WBSM Rada</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Rada</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure> -->
			<WBSMMeasure>
				<Label>WBSM Resnik</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>Resnik</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM WuPalmerFast</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>WuPalmerFast</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM JiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>JiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM CosNormJConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>CosineNormJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<!--<WBSMMeasure>
				<Label>WBSM WeightedJiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>WeightedJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>
			<WBSMMeasure>
				<Label>WBSM CosineNormWeightedJiangConrath</Label>
				<Method>WBSMMeasure</Method>
				<WordNetDbDir>../Wordnet-3.0/dict</WordNetDbDir>
				<WordNetDbFilename>data.noun</WordNetDbFilename>
				<WordSimilarityMeasureType>CosineNormWeightedJiangConrath</WordSimilarityMeasureType>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>nltk2018StopWords.txt</StopWordsFilename>
					<TokenizerType>WhiteSpace</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
				</WordProcessing>
			</WBSMMeasure>-->
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
</SentenceSimilarityExperiments>